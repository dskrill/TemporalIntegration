{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from word_swap import *\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModel,pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining and preprocessing a corpus\n",
    "\n",
    "For simplicity of demonstration, we use the Brown Corpus, conveniently available from NLTK. The Brown Corpus is a collection of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961. The corpus is divided into 15 genres, such as news, editorial, and fiction; for this demonstration, we use the entire corpus. The files are further divided into sentences and tagged with part-of-speech (POS) tags. For more information on the Brown Corpus, see the [NLTK documentation](http://www.nltk.org/book/ch02.html#brown-corpus).\n",
    "\n",
    "The following code imports the Brown Corpus preprocesses each sentence. The preprocessing:\n",
    "- Removes punctuation\n",
    "- Converts all words to lowercase\n",
    "\n",
    "Additionally, this step removes sentences containing proper nouns or non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_natural_sequences(tagged_words,sequence_len):\n",
    "#     \"\"\"Extracts natural sequences of specified length from a list of pos-tagged words. \n",
    "\n",
    "#     Additionally preprocesses the words by removing punctuation and lowercasing, and builds\n",
    "#     up a dictionary of words to their POS tags.\n",
    "\n",
    "#     Args:\n",
    "#         tagged_words: a list of tuples (word,tag)\n",
    "#         sequence_len: the length of the sequences to extract\n",
    "\n",
    "#     Returns:    \n",
    "#         a list of natural sequences of the specified length\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     natural_sequences = []\n",
    "#     word_list = []\n",
    "#     counter = 0\n",
    "\n",
    "#     for word,tag in tagged_words:\n",
    "#         w = word.replace(\"-\",\"\").replace(\"''\",\"\").lower().translate(translator)\n",
    "#         if w.isalpha() and w != '' and not (tag in excluded_pos):\n",
    "#             word_list.append(w)\n",
    "#             counter += 1\n",
    "#             if counter == sequence_len:\n",
    "#                 natural_sequences.append(word_list[-sequence_len:])\n",
    "#                 counter = 0\n",
    "#         else:\n",
    "#             counter = 0\n",
    "    \n",
    "#     return natural_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_sentence(s,single_token_words=False,tokenizer=None, excluded_token_ids=[]):\n",
    "#     \"\"\"Preprocess a sentence by removing punctuation and lowercasing.\n",
    "    \n",
    "#     Additionally filters sentences that contain proper nouns or \n",
    "#     non-alphabetic characters.\n",
    "    \n",
    "#     Args:\n",
    "#         s: a list of strings (e.g., [\"I\", \"like\", \"apples.\"])\n",
    "        \n",
    "#     Returns:\n",
    "#         A list of words (e.g., [\"i\", \"like\", \"apples\"]) or None if\n",
    "#         the sentence should be filtered out.\n",
    "#     \"\"\"\n",
    "\n",
    "#     assert not (single_token_words and tokenizer is None), \"Must provide a tokenizer if single_token_words is True\"\n",
    "\n",
    "#     tags = [tag for (word,tag) in nltk.pos_tag(s)]\n",
    "#     # check for proper nouns\n",
    "    \n",
    "#     if any(tag in excluded_pos for tag in tags):\n",
    "#         return None\n",
    "#     else:\n",
    "#         # separate hypenated words and remove quotes\n",
    "#         new = \" \".join(s).replace(\"-\",\" \").replace(\"''\",\"\").lower()\n",
    "#         # remove punctuation\n",
    "#         split = new.translate(translator).split()\n",
    "#         split = [s for s in split if s != \"''\"]\n",
    "#         # check if all words are alphabetic\n",
    "#         if \"\".join(split).isalpha():\n",
    "#             if single_token_words:\n",
    "#                 # only add sentences that are exactly sentence_len tokens long\n",
    "#                 tokenized = [x for x in tokenizer.encode(\" \".join(split)) if x not in excluded_token_ids]\n",
    "#                 if len(tokenized) == len(split):\n",
    "#                     return split\n",
    "#                 else:\n",
    "#                     return None\n",
    "#             return split\n",
    "#         else:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Corpus:\n",
    "#     def __init__(self,nltk_corpus,single_token_words=False,tokenizer=None,excluded_token_ids=[]):\n",
    "#         self.corpus = nltk_corpus\n",
    "#         self.single_token_words = single_token_words\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.excluded_token_ids = excluded_token_ids\n",
    "#         self.all_preprocessed_sentences = []\n",
    "\n",
    "#         assert not (single_token_words and tokenizer is None), \"If single_token_words is True, a tokenizer must be provided.\"\n",
    "#         if tokenizer:\n",
    "#             assert tokenizer.add_prefix_space, \"The tokenizer must have add_prefix_space set to True.\"\n",
    "\n",
    "#         for s in tqdm(nltk_corpus.sents()):\n",
    "#             new = process_sentence(s,single_token_words,tokenizer,excluded_token_ids)\n",
    "#             if new is not None:\n",
    "#                 self.all_preprocessed_sentences.append(new)\n",
    "#         self.word_lookup = {}\n",
    "#         self.pos_dict = {}\n",
    "#         self.tagged_words_no_punct = []\n",
    "#         for w,t in self.corpus.tagged_words():\n",
    "#             wp = w.translate(translator)\n",
    "#             if wp.isalpha():\n",
    "#                 self.tagged_words_no_punct.append((wp,t))\n",
    "#         for word,tag in tqdm(self.tagged_words_no_punct):\n",
    "#             w = word.replace(\"-\",\"\").replace(\"''\",\"\").lower().translate(translator)\n",
    "#             if w.isalpha() and w != '' and not (tag in excluded_pos):\n",
    "#                 if tag in self.pos_dict:\n",
    "#                     self.pos_dict[tag].append(w)\n",
    "#                 else:\n",
    "#                     self.pos_dict[tag] = [w]\n",
    "#                 self.word_lookup[w] = tag\n",
    "\n",
    "\n",
    "#         if single_token_words:\n",
    "#             temp_pos_dict = {'OTHER':[]}\n",
    "#             for key,value in pos_dict.items():\n",
    "#                 new = []\n",
    "#                 for v in set(value):\n",
    "#                     token = [t for t in tokenizer.encode(v) if t not in excluded_token_ids]\n",
    "#                     if len(token) == 1:\n",
    "#                         new.append(v)\n",
    "\n",
    "#                 if len(set(new)) < 5:\n",
    "#                     temp_pos_dict['OTHER'].extend(set(new))\n",
    "#                 else:\n",
    "#                     temp_pos_dict[key] = new\n",
    "\n",
    "#             temp_word_lookup = {}\n",
    "#             for pos in temp_pos_dict:\n",
    "#                 for word in temp_pos_dict[pos]:\n",
    "#                     temp_word_lookup[word] = pos\n",
    "#             self.pos_dict = temp_pos_dict\n",
    "#             self.word_lookup = temp_word_lookup\n",
    "\n",
    "#     def get_sentences_of_length(self, length):\n",
    "#         return [s for s in self.all_preprocessed_sentences if len(s) == length]\n",
    "\n",
    "#     def get_natural_sequences_of_length(self, length):\n",
    "#         natural_sequences = extract_natural_sequences(\n",
    "#             tagged_words = self.corpus.tagged_words(),\n",
    "#             sequence_len = length\n",
    "#         )\n",
    "#         return natural_sequences\n",
    "\n",
    "#     def get_noun_phrases_of_length(self, length):\n",
    "#         noun_phrases = extract_noun_phrases(\n",
    "#             self.all_preprocessed_sentences,\n",
    "#             phrase_length = length)\n",
    "#         return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cce01f688643b1983fae7f9de46e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcb7e1ca794401cbcc68635c070db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1004244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = Corpus(brown)\n",
    "# all_preprocessed_sentences = []\n",
    "# for s in tqdm(nltk.corpus.brown.sents()):\n",
    "#     new = process_sentence(s)\n",
    "#     if new is not None:\n",
    "#         all_preprocessed_sentences.append(new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter the preprocessed sentences to retain only those having particular lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences = corpus.get_sentences_of_length(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we inspect the first five 12-word sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['under committee rules it went automatically to a subcommittee for one week',\n",
       " 'nothing has been done yet to take advantage of the enabling legislation',\n",
       " 'how can a man with any degree of common decency charge this',\n",
       " 'he then launched into what the issues should be in the campaign',\n",
       " 'there is a tangible feeling in the air of revulsion toward politics']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(selected_sentences[i]) for i in range(5)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also similarly extract and preprocess sequences of natural text (not necessarilly delimited by sentence boundaries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_sequences = corpus.get_natural_sequences_of_length(12)\n",
    "# natural_sequences,word_lookup,pos_dict = extract_natural_sequences(\n",
    "#     tagged_words = nltk.corpus.brown.tagged_words(),\n",
    "#     sequence_len = 40\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 the:AT\n",
      "1 jury:NN\n",
      "2 further:RBR\n",
      "3 said:VBD\n",
      "4 in:IN\n",
      "5 termend:NN\n",
      "6 presentments:NNS\n",
      "7 that:CS\n",
      "8 the:AT\n",
      "9 city:NN-TL\n",
      "10 executive:NN\n",
      "11 committee:NN-TL\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(natural_sequences[0]):\n",
    "    print(f\"{i} {word}:{corpus.word_lookup[word]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Corpus object also has a dictionary that allows one to lookup words having a given POS-tag, e.g., adjectives (`JJ`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recent',\n",
       " 'overall',\n",
       " 'possible',\n",
       " 'hardfought',\n",
       " 'relative',\n",
       " 'such',\n",
       " 'widespread',\n",
       " 'outmoded',\n",
       " 'inadequate',\n",
       " 'ambiguous']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.pos_dict[\"JJ\"][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, one can similarly extract noun phrases from the corpus. For convenience, we repurpose the preprocessed sentences obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrases = corpus.get_noun_phrases_of_length(8)\n",
    "\n",
    "# noun_phrases = extract_noun_phrases(\n",
    "#     sum(selected_sentences.values(), []),\n",
    "#     phrase_length = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the entire two and a half mile length',\n",
       " 'the same three month period toll road bonds',\n",
       " 'the only or even the most appropriate conceptuality',\n",
       " 'the intellectual social political and economic attitudes institutions',\n",
       " 'the true color texture complexity range intensity pulse']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(noun_phrases[i]) for i in range(5)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word swap procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the corpus has been preprocessed, we can create sequences and perform the word swap procedure. The nature of the input sequences and the method of selecting the swapped word vary with the analysis at hand, but the word swap procedure remains the same. Specifically:\n",
    "\n",
    "- To assess overall integration, we use sequences of natural text(e.g., of 40 words in length), and make swaps using randomly selected words having the same POS-tag.\n",
    "- To assess structure yoking, we create sequences of five structures of prespecified length (e.g., 12-word sentences), and make swaps from a pool of words selected based on the difference in the embeddings betwen the original and candidate words.\n",
    "- We also consider making swaps using probable (or improbable) words, as determined (independently) by BERT.\n",
    "\n",
    "We found that sentence-final words yielded slightly more tokens on average than words in other positions. To account for this in the structure yoking analysis, we tokenize all candidate sentences and used only those that are comprised entirely of single-token words.\n",
    "\n",
    "One can instantiate a tokenizer as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model, add_special_token=False,add_prefix_space=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Corpus also needs to be reinitialized with the new tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934c0bc127ba4edd98289f01517fe9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e29f9d64d93433192d9c245d8e8d116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1004244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = Corpus(brown,single_token_words=True,tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 does not have any structure-delimiting tokens. Other models (e.g., BERT) do have such tokens, which need to be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoking_sequences = create_yoking_sequences(\n",
    "    sentences = corpus.get_sentences_of_length(12),\n",
    "    tokenizer = tokenizer,\n",
    "    excluded_token_ids = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and to develop a discriminating audience for good drama and sensitive performance they are not true because scientists or prophets say they are true'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(yoking_sequences[0][:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSwap:\n",
    "    def __init__(self,excluded_token_ids=[]):\n",
    "        self.original_sequences = []\n",
    "        self.swapped = []\n",
    "        self.excluded_token_ids = excluded_token_ids \n",
    "\n",
    "        self.ok = True\n",
    "\n",
    "    def __call__(self,sequences):\n",
    "        pass\n",
    "\n",
    "class RandomWordSwap(WordSwap):\n",
    "    def __init__(self,excluded_token_ids=[]):\n",
    "        super().__init__(excluded_token_ids=excluded_token_ids)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Word swaps using random choice from word list\"\n",
    "\n",
    "    def __call__(self,sequences):\n",
    "        n_sequences = len(sequences)\n",
    "\n",
    "        for sequence_num,s in tqdm(enumerate(sequences)):\n",
    "            other = sequences[np.random.choice(range(n_sequences),1)[0]]\n",
    "            swapped_sequences = []\n",
    "            for i,word in enumerate(s):\n",
    "                temp = s.copy()\n",
    "                to_swap = random.choice(word_list)\n",
    "                while to_swap == word:\n",
    "                    to_swap = random.choice(word_list)\n",
    "                temp[i] = to_swap\n",
    "                swapped_sequences.append(temp)\n",
    "\n",
    "            self.swapped.append(swapped_sequences)\n",
    "            self.original_sequences.append(s)\n",
    "\n",
    "class RandomPosWordSwap(WordSwap):\n",
    "    def __init__(self,word_lookup,pos_dict,tokenizer,excluded_token_ids = []):\n",
    "        super().__init__(excluded_token_ids=excluded_token_ids)\n",
    "        self.word_lookup = word_lookup\n",
    "        self.pos_dict = pos_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_list = []\n",
    "        for w in list(word_lookup.keys()):\n",
    "            tokens = [t for t in tokenizer.encode(w) if t not in excluded_token_ids]\n",
    "            if len(tokens) == 1:\n",
    "                self.word_list.append(w)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Word swaps using random choice from same POS\"\n",
    "\n",
    "    def __call__(self,sequences):\n",
    "        for sequence_num,s in tqdm(enumerate(sequences)):\n",
    "            swapped_sequences = []\n",
    "            for i,word in enumerate(s):\n",
    "                temp = s.copy()\n",
    "                current_pos = self.word_lookup[word]\n",
    "                if current_pos not in self.pos_dict:\n",
    "                    print(\"Warning: \",current_pos,\" not in pos_dict\")\n",
    "                    print(word)\n",
    "                    to_swap = random.choice(self.word_list)\n",
    "                else:\n",
    "                    to_swap = random.choice(self.pos_dict[current_pos])\n",
    "                try_count = 0\n",
    "                token = self.tokenizer.encode(to_swap)\n",
    "                token = [t for t in token if t not in self.excluded_token_ids]\n",
    "                while (to_swap == word) or len(token)>1:\n",
    "                    if (try_count < 10) and (current_pos in self.pos_dict):\n",
    "                        to_swap = random.choice(self.pos_dict[current_pos]) # some rare pos tags may not have many words\n",
    "                        token = self.tokenizer.encode(to_swap)\n",
    "                        token = [t for t in token if t not in self.excluded_token_ids]\n",
    "                        try_count += 1\n",
    "                        # print(try_count,current_pos)\n",
    "                        if try_count == 10:\n",
    "                            print(f\"Warning: {current_pos} has few words (current word: {word})\")\n",
    "                            break\n",
    "                    else:\n",
    "                        to_swap = random.choice(self.word_list) # just pick randomly in that case\n",
    "                        break\n",
    "                temp[i] = to_swap\n",
    "                swapped_sequences.append(temp)\n",
    "            self.swapped.append(swapped_sequences)\n",
    "            self.original_sequences.append(s)\n",
    "\n",
    "\n",
    "\n",
    "pos_swapper = RandomPosWordSwap(corpus.word_lookup,corpus.pos_dict,tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling them on an appropriate list of sequences, the word swap objects have attributes `swapped` and `original_sequences` that contain the swapped and original sequences, respectively. `swapped` is a list of lists: `swapped[i][j]` if the `i`th sequence with the `j`th word swapped. Indices are zero-indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: and to develop a discriminating audience for good drama and\n",
      "Swapped at 5: and to develop a discriminating drafting for good drama and\n",
      "Swapped at 6: and to develop a discriminating audience to good drama and\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 5\n",
    "print(\"Original: \" + \" \".join(pos_swapper.original_sequences[i][:10])),\n",
    "print(f\"Swapped at {j}: \" + \" \".join(pos_swapper.swapped[i][j][:10])) \n",
    "print(f\"Swapped at {j+1}: \" + \" \".join(pos_swapper.swapped[i][j+1][:10])) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see how the other swap procedures work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114c55b2249649889c933ff6fdc0a0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate embeddings created of shape:  torch.Size([14999, 768])\n"
     ]
    }
   ],
   "source": [
    "def create_candidate_embeddings(tokens,embedding_function):\n",
    "    with torch.no_grad():\n",
    "        candidate_embeddings = embedding_function(torch.stack(tokens)).squeeze()\n",
    "    print(\"Candidate embeddings created of shape: \",candidate_embeddings.shape)\n",
    "    return candidate_embeddings\n",
    "\n",
    "def filter_word_list(word_list,tokenizer,excluded_token_ids = []):\n",
    "    wlist = [w for w in word_list if (w.isalpha())]\n",
    "    out = []\n",
    "    tokens = []\n",
    "    for word in tqdm(wlist):\n",
    "        current = tokenizer.encode(word)\n",
    "        current = torch.tensor([q for q in current if q not in excluded_token_ids])\n",
    "        if len(current) == 1:\n",
    "            out.append(word)\n",
    "            tokens.append(current)\n",
    "    return out,tokens\n",
    "\n",
    "word_list = list(corpus.word_lookup.keys())\n",
    "word_list,tokens = filter_word_list(word_list, tokenizer,[])\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'gpt2', add_special_token=False,add_prefix_space=True)\n",
    "\n",
    "def create_distribution(word,candidate_embeddings,mean,half_width,embedding_function, excluded_token_ids = [], size=10,spacing=.001):\n",
    "    out = None\n",
    "    with torch.no_grad():\n",
    "        token = tokenizer.encode(word)\n",
    "        token = torch.tensor([t for t in token if t not in excluded_token_ids])\n",
    "\n",
    "        target = embedding_function(token).mean(0,keepdim=True)\n",
    "\n",
    "        assert target.ndim == 2\n",
    "        diff = torch.abs(candidate_embeddings - target).mean(-1)\n",
    "        for c in torch.arange(mean-half_width,mean+half_width,spacing):\n",
    "            indices = torch.nonzero((diff > c) & (diff < c+spacing),as_tuple=True)[0]\n",
    "            ridx = np.random.choice(len(indices),size=size,replace=True)\n",
    "            if out is None:\n",
    "                out = indices[ridx]\n",
    "            else:\n",
    "                out = torch.cat([out,indices[ridx]])\n",
    "        return out\n",
    "\n",
    "class DistributionWordSwap(WordSwap):\n",
    "    def __init__(self,word_list,embedding_function,sampling_params,excluded_token_ids = []):\n",
    "        super().__init__(excluded_token_ids=excluded_token_ids)\n",
    "        self.word_list = word_list\n",
    "        self.embedding_function = embedding_function\n",
    "        self.sampling_params = sampling_params\n",
    "\n",
    "        self.candidate_embeddings = create_candidate_embeddings(tokens,embedding_function)\n",
    "        assert candidate_embeddings.shape[0] == len(word_list), \"Word list and candidate embeddings don't match\"\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Word swaps with embedding differences sampled from a uniform distribution\"\n",
    "\n",
    "    def __call__(self,sequences):\n",
    "        potential_swaps = {}\n",
    "        all_diffs = {}\n",
    "        for sequence_num,s in tqdm(enumerate(sequences)):\n",
    "            swapped_sequences = []\n",
    "            for i,word in enumerate(s):\n",
    "                if self.ok:\n",
    "                    temp = s.copy()\n",
    "                    try:\n",
    "                        distribution = create_distribution(word,\n",
    "                                                            candidate_embeddings,\n",
    "                                                            **self.sampling_params,\n",
    "                                                            embedding_function = self.embedding_function,\n",
    "                                                            excluded_token_ids = self.excluded_token_ids)\n",
    "\n",
    "                        selected_idx = np.random.choice(distribution)\n",
    "                        temp[i] = word_list[selected_idx]\n",
    "                    except:\n",
    "                        print(\"Using random word sub for: \",word)\n",
    "                        temp[i] = random.choice(word_list)\n",
    "                    swapped_sequences.append(temp)\n",
    "            self.swapped.append(swapped_sequences)\n",
    "            self.original_sequences.append(s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# candidate_embeddings = create_candidate_embeddings(tokens,model.base_model.wte.forward)\n",
    "sampling_params = {'mean':.125,'half_width':.003,'spacing':.001,'size':100}\n",
    "distribution_swapper = DistributionWordSwap(word_list,\n",
    "                                            model.base_model.wte.forward,\n",
    "                                            sampling_params,\n",
    "                                            excluded_token_ids = [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a65f3c3d9e4b389ef05740a0f88bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_swapper(yoking_sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: and to develop a discriminating audience for good drama and\n",
      "Swapped at 5: and to develop a discriminating integration for good drama and\n",
      "Swapped at 6: and to develop a discriminating audience whence good drama and\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: \" + \" \".join(distribution_swapper.original_sequences[i][:10])),\n",
    "print(f\"Swapped at {j}: \" + \" \".join(distribution_swapper.swapped[i][j][:10])) \n",
    "print(f\"Swapped at {j+1}: \" + \" \".join(distribution_swapper.swapped[i][j+1][:10])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2606211bf84e008ebed7b56b3ec3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ProbableWordSwap(WordSwap):\n",
    "    def __init__(self,unmasker,idx_low = 0,idx_high = 100,sentence_len = None,excluded_token_ids = []):\n",
    "        super().__init__(excluded_token_ids)\n",
    "        self.unmasker = unmasker\n",
    "        self.idx_low = idx_low\n",
    "        self.idx_high = idx_high\n",
    "        self.sentence_len = sentence_len\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Word swaps sampled from a probable pool of words (determined using a user-specified unmasker)\"\n",
    "\n",
    "    def __call__(self,sequences):\n",
    "        for sequence_num,s in tqdm(enumerate(sequences)):\n",
    "            try:\n",
    "                swapped_sequences = []\n",
    "                for i,word in enumerate(s):\n",
    "                    temp = s.copy()\n",
    "                    temp[i] = '[MASK]'\n",
    "                    if self.sentence_len:\n",
    "                        current_sentence_start = (i//self.sentence_len)*self.sentence_len\n",
    "                        current_sentence_end = current_sentence_start + self.sentence_len\n",
    "                        current_sentence = \" \".join(temp[current_sentence_start:current_sentence_end])\n",
    "                    else:\n",
    "                        current_sentence = \" \".join(temp)\n",
    "                    \n",
    "                    potential_swaps = np.array([x[\"token_str\"] for x in self.unmasker(current_sentence) if ((x[\"token_str\"] != word) and x[\"token_str\"].isalpha())])\n",
    "                    n_potential_swaps = len(potential_swaps)\n",
    "                    temp[i] = potential_swaps[np.random.randint(low=self.idx_low,high=min(self.idx_high,n_potential_swaps),size=1)[0]] \n",
    "                    swapped_sequences.append(temp)\n",
    "            except Exception as e:\n",
    "                self.ok = False\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if self.ok:\n",
    "                self.swapped.append(swapped_sequences)\n",
    "                self.original_sequences.append(s)\n",
    "\n",
    "unmasker = pipeline('fill-mask',model = 'bert-large-uncased-whole-word-masking',top_k=1000,device='cpu')\n",
    "probable_swapper = ProbableWordSwap(unmasker,idx_low = 0,idx_high = 5,sentence_len = 12,excluded_token_ids = [])\n",
    "probable_swapper(yoking_sequences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: and to develop a discriminating audience for good drama and\n",
      "Swapped at 5: and to develop a discriminating taste for good drama and\n",
      "Swapped at 6: and to develop a discriminating audience of good drama and\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: \" + \" \".join(probable_swapper.original_sequences[i][:10])),\n",
    "print(f\"Swapped at {j}: \" + \" \".join(probable_swapper.swapped[i][j][:10])) \n",
    "print(f\"Swapped at {j+1}: \" + \" \".join(probable_swapper.swapped[i][j+1][:10])) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
